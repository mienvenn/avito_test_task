# Тестовое задание Авито

## Курманаева Эвелина

### Структура проекта

*  `AVITO_test.ipynb` — это файл обучения модели
*  `inference.ipynb` — это файл для инференса
*  `README.md` — описание подхода к решению

### Датасет

Датасет, на котором обучалась модель, взят по этой ссылке: https://www.kaggle.com/datasets/vitaliy3000/avito-dataset?select=train.csv

Использовался данный датасет, так как он похож по стилю, формулировкам и структуре предложений с датасетом `dataset_1937770_3.txt` . Его легко собрать, достаточно создать колонку `input`, просто удалив пробелы в правильном тексте.

### Основная идея

На вход модели подается текст без пробелов, и она классифицирует каждый токен как `0` (не вставлять пробел) или `1` (ставить пробел после токена), то есть выполняется задача последовательной бинарной классификации (sequence labeling). После этого с помощью функции `restore_spaces` восстанавливается текст с пробелами на местах, где модель предсказала `1`.

### Токенизация

Токенизация выполняется с помощью **Byte-Pair Encoding (BPE)**, который разбивает текст на подслова, а не на отдельные символы. Это позволяет сократить длину последовательности, ускорить обучение, экономить память и сохранить морфологическую информацию. В отличие от посимвольного токенизатора, BPE учитывает статистику сочетаний символов, что помогает модели лучше понимать морфологию слов и их границы. Этот вывод был сделан при сравнении BPE и char-encoding.

Так как изначальная маска посимвольная, то она не подходит при использовании BPE. Поэтому с помощью функции `make_labels` формируются **новые маски**. Для каждого текста каждому токену присваивается метка из исходной маски. С помощью `offset_mapping` узнаём, какие символы входят в токен, и берём значение из маски для последнего символа токена (`0` — пробел не ставить, `1` — ставить).

### Модель

В работе используется [**модель XLM-RoBERTa**](https://huggingface.co/FacebookAI/xlm-roberta-base) — улучшенная версия BERT, обученная на большом многоязычном корпусе. Она обрабатывает последовательность токенов и учитывает связи между ними в обе стороны. Благодаря механизму self-attention модель может выделять наиболее значимые токены в контексте всей фразы и улавливать синтаксические закономерности. 

### Почему этот метод решения лучше эвристики

Было решено, что для задачи восстановления пробелов **эвристика плохо работает**, потому что входные данные — не всегда чистый текст: пользователь может опечататься, писать в разном стиле (полностью заглавными буквами, чередуя заглавные с прописными и т.д.). Эвристики с помощью словаря не могут учесть все варианты, потому что список слов не может быть полным. Кроме того, решение о том, нужен ли пробел, зависит от смысла, а не только от формы слова.

### Проблема дисбаланса классов

При решении задачи наблюдается сильный **дисбаланс классов**: меток `0` больше, чем меток `1`. Если обучать модель без учета этого дисбаланса, она будет склоняться к тому, чтобы всегда предсказывать `0`, что приведет к низкому качеству восстановления текста. Чтобы компенсировать дисбаланс, я использовала взвешенную функцию потерь: веса для каждого класса вычислялись как обратные их частотам. 

### Обучение модели

Для обучения модели используется **пакетная обработка данных**, при которой длины последовательностей внутри батча выравниваются с помощью функции `collate_fn`. Для каждой выборки определяется максимальная длина последовательности, и все остальные последовательности дополняются до этой длины. `labels` выравниваются значением `-100`, чтобы паддинги не влияли на обучение при вычислении функции потерь.

### Мощности для обучения 

Обучение и инференс выполнялись на **MacBook M1 Pro с поддержкой MPS**
